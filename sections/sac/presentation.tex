Nous décidons d'implémenter un algorithme \emph{state of the art}. Nous avons donc le choix entre les algorithme D4PG, TD3, TRPO et SAC. Nous choisissons l'algorithme \emph{Soft Actor Critic} parce qu'il présente un bon compromis entre TRPO qui est stable mais qui sous exploite ses échantillons, et TD3 qui est instable et exploite au mieux les échantillons. SAC, quant à lui, réunit le meilleur des deux mondes en étant stable et utilisant efficacement un \emph{replay buffer}. La gestion de l'entropie sur le domaine des actions permet de favoriser l'exploration et d'éviter une politique stochastique. L'utilisation de deux critiques le plus résistant à l'\emph{over-estimation biais}.

Nous avons trouvé un \href{https://github.com/seungeunrho/minimalRL/blob/master/sac.py}{dépôt sur Github} qui propose une implémentation relativement légère et simple à comprendre rédigée par \href{https://github.com/seungeunrho}{seungeunrho}. Celle-ci est proche de celle proposé par \href{https://spinningup.openai.com/en/latest/algorithms/sac.html}{Spinning Up}. Cependant, cette version de l'algorithme apprend aussi le paramètre $\alpha$.

Nous avons restructuré ce code et y avons incorporé la visualisation du dépôt donné pour ce TME. Nous nous somme assuré que la politique générée puisse être utilisée dans \emph{evaluator.py}. Enfin, nous avons changé les arguments du programme pour qu'ils correspondent aux hyper-paramètres.