Nous décidons d'implémenter un algorithme \emph{state of the art}. Nous avons donc le choix entre les algorithmes D4PG, TD3, TRPO et SAC. Nous choisissons l'algorithme \emph{Soft Actor Critic} parce qu'il présente un bon compromis entre TRPO qui est stable mais qui sous exploite ses échantillons, et TD3 qui est instable et exploite au mieux les échantillons. SAC réunit le meilleur des deux mondes en étant stable et en utilisant efficacement un \emph{replay buffer}. La gestion de l'entropie sur le domaine des actions permet de favoriser l'exploration et d'éviter une politique stochastique. L'utilisation de deux critiques le rend plus résistant à l'\emph{over-estimation bias}.

Nous avons trouvé un \href{https://github.com/seungeunrho/minimalRL/blob/master/sac.py}{dépôt sur Github} qui propose une implémentation relativement légère et simple à comprendre rédigée par \href{https://github.com/seungeunrho}{seungeunrho}. Celle-ci est proche de celle proposée par \href{https://spinningup.openai.com/en/latest/algorithms/sac.html}{Spinning Up}. Cependant, cette version de l'algorithme apprend aussi le paramètre $\alpha$.

Nous avons restructuré ce code et y avons incorporé la visualisation du dépôt donné pour ce TME. Nous nous sommes assuré que la politique générée puisse être utilisée dans \emph{evaluator.py}. Enfin, nous avons changé les arguments du programme pour qu'ils correspondent aux hyper-paramètres.